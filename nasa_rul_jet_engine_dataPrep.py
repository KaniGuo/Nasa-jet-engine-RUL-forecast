# -*- coding: utf-8 -*-
"""NASA_RUL_jet_engine

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14AdifW97cnRlachQwLvueM4zIsmvr3yB

**Predictive maintenance** - *A management technique that uses regular evaluation of the actual operating condition of plant equipment, production systems and plant management functions to optimize total plant operation.*
"""
#%%
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import os

"""- I would suggest to create a folder named "Data" in "Files" and upload "RUL", "test", "train" files there. So that we would have the same cofiguration.
- After copying the path of the first file, I have "/content/Data/RUL_FD001.txt"


### Sensors data

"""
# -------------------------------
## PARAMS TO MODIFY BEFORE RUNNING
# -------------------------------
plotting = False # PLOT DATA SKEWNESS / VARIANCE / MEAN of each sensor!

## Sequence parameters 
sequence_lengths = [52]
for sequence_length in sequence_lengths:
  print(sequence_length)
  # sequence_length = 26
  forecast_length = 1
  sequence_step = 1 
  # -------------------------------


  columns1=["unit_nr","cycle","op_sett1","op_sett2","op_sett3"] + [f'sensor{i}' for i in range(1, 22)]

  columns2=["","","","","",'T2','T24','T30','T50','P2','P15','P30','Nf',
            'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']

  columns = [columns1[i]+'_'+columns2[i] for i in range(len(columns1))]

  #%%

  path = 'Data'

  file001 = r'Data\train_FD001.txt'
  file002 = r'Data\train_FD002.txt'
  file003 = r'Data\train_FD003.txt'
  file004 = r'Data\train_FD004.txt'

  train_fd1 = pd.read_csv(file001, sep=' ', header=None)
  train_fd2 = pd.read_csv(file002, sep=' ', header=None)
  train_fd3 = pd.read_csv(file003, sep=' ', header=None)
  train_fd4 = pd.read_csv(file004, sep=' ', header=None)

  train_fd1.head()

  #%%
  # Check whether columns 26&27 in all datasets are empty
  for data in train_fd1, train_fd2, train_fd3, train_fd4:
    print(data[[26, 27]].head())

  train_fd1.isnull().sum()

  # Drop columns 26 & 27 - they are empty
  for data in train_fd1, train_fd2, train_fd3, train_fd4:
    data.drop(columns=[26, 27], inplace=True)
    data.columns=columns

  train_fd1.head()

  train_fd1.describe().T

  """- From above it can be seen that the following sensors do not have any useful information:

  *   sensor1_T2
  *   sensor5_P2
  *   sensor6_P15
  *   sensor16_farB
  *   sensor19_PCNfR_dmd
  """

  # Function to add Remaining Useful Life (RUL) in training dataset.

  def add_rul(df):
    """Add RUL column and values in the training data"""
    new_df = df.copy()
    new_df['RUL'] = np.zeros(new_df.shape[0])

    for i in df.groupby(df[df.columns[0]]): # returs tuple -> (id, data)
      row_index = np.where(df[df.columns[0]] == i[0])[0]
      # number of cycles before failure
      cycles = i[1][df.columns[1]].max()
      # creating reverse list of remaining cycles after each cycle
      values = np.arange(cycles-1, -1, -1)
      for row, value in zip(row_index, values):
        new_df.loc[row, 'RUL'] = 125 if value > 125 else value

    return new_df

  # train_fd1.head()

  fd001 = add_rul(train_fd1)
  fd002 = add_rul(train_fd2)
  fd003 = add_rul(train_fd3)
  fd004 = add_rul(train_fd4)


  # fd001.columns

  fd001[fd001.columns[5:-1]].std().plot.barh(x='Deviation')
  # fd002[fd002.columns[5:-1]].std().round(3).plot.barh(x='Deviation')
  # fd003[fd003.columns[5:-1]].std().round(3).plot.barh(x='Deviation')
  # fd004[fd004.columns[5:-1]].std().round(3).plot.barh(x='Deviation')

  print(*enumerate(columns), sep='\n')

  #%%
  # --------------
  # Plotting
  # --------------
  if plotting == True:
    n_rows = 21
    n_cols = 3
    fig_width = 16
    fig_height = n_rows * 2

    # Variables to play with
    alpha = 0.5
    id = columns[0]
    sensors = columns[5:]

    fig, ax = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height), sharex=False, tight_layout=True)
    for i in range(len(sensors)):
      sensor = sensors[i]
      # FD001
      variance_001 = [x[1][sensor].var() for x in fd001.groupby(id)]
      mean_001 = [x[1][sensor].mean() for x in fd001.groupby(id)]
      skew_001 = [x[1][sensor].skew() for x in fd001.groupby(id)]
      # FD002
      variance_002 = [x[1][sensor].var() for x in fd002.groupby(id)]
      mean_002 = [x[1][sensor].mean() for x in fd002.groupby(id)]
      skew_002 = [x[1][sensor].skew() for x in fd002.groupby(id)]
      # FD003
      variance_003 = [x[1][sensor].var() for x in fd003.groupby(id)]
      mean_003 = [x[1][sensor].mean() for x in fd003.groupby(id)]
      skew_003 = [x[1][sensor].skew() for x in fd003.groupby(id)]
      # FD004
      variance_004 = [x[1][sensor].var() for x in fd004.groupby(id)]
      mean_004 = [x[1][sensor].mean() for x in fd004.groupby(id)]
      skew_004 = [x[1][sensor].skew() for x in fd004.groupby(id)]


      ax[i-1, 0].scatter(mean_001, skew_001, alpha=alpha, label='FD001-HPC Degradation-One')
      ax[i-1, 0].scatter(mean_002, skew_002, alpha=alpha, label='FD002-HPC Degradation-Six')
      ax[i-1, 0].scatter(mean_003, skew_003, alpha=alpha, marker='.', label='FD003-HPC + FAN-One')
      ax[i-1, 0].scatter(mean_004, skew_004, alpha=alpha, marker='.', label='FD004-HPC + FAN-Six')
      ax[i-1, 0].set_ylabel('Skewness')
      ax[i-1, 0].set_xlabel('Mean')
      #
      ax[i-1, 1].scatter(mean_001, variance_001, alpha=alpha, label='FD001-HPC Degradation-One')
      ax[i-1, 1].scatter(mean_002, variance_002, alpha=alpha, label='FD002-HPC Degradation-Six')
      ax[i-1, 1].scatter(mean_003, variance_003, alpha=alpha, marker='.', label='FD003-HPC + FAN-One')
      ax[i-1, 1].scatter(mean_004, variance_004, alpha=alpha, marker='.', label='FD004-HPC + FAN-Six')
      ax[i-1, 1].set_ylabel('Variance')
      ax[i-1, 1].set_xlabel('Mean')
      ax[i-1, 1].set_title(f'{sensor}')
      if i % 3 == 0:
        ax[i-1, 1].legend()
      #
      ax[i-1, 2].scatter(skew_001, variance_001, alpha=alpha, label='FD001-HPC Degradation-One')
      ax[i-1, 2].scatter(skew_002, variance_002, alpha=alpha, label='FD002-HPC Degradation-Six')
      ax[i-1, 2].scatter(skew_003, variance_003, alpha=alpha, marker='.', label='FD003-HPC + FAN-One')
      ax[i-1, 2].scatter(skew_004, variance_004, alpha=alpha, marker='.', label='FD004-HPC + FAN-Six')
      ax[i-1, 2].set_ylabel('Variance')
      ax[i-1, 2].set_xlabel('Skewness')

  # import os; os.listdir()

  #%%
  """### Drop columns"""

  # Filtering columns - if STD is lower than 0.01 these columns will be dropped
  clm_to_drop = train_fd1.columns[np.where(train_fd1.std() < 0.01)] # to drop

  # Useful columns
  clm_to_use = train_fd1.columns[np.where(train_fd1.std() > 0.01)] # useful
  # clm_to_use = clm_to_use[2:]
  print('Drop:', clm_to_drop)
  print('Useful:', clm_to_use)

  # Data slicing
  x_train_001 = fd001[clm_to_use]
  x_train_003 = fd003[clm_to_use]

  # Target data
  y_train_001 = fd001.RUL
  y_train_003 = fd003.RUL

  x_train_001.describe().T

  # fd001.head()

  #%%
  """### Scaling"""

  from sklearn.preprocessing import StandardScaler

  scaler_001 = StandardScaler()
  scaler_003 = StandardScaler()

  # for trainset FD001, only do on the sensor columns
  x_train_001 = scaler_001.fit_transform(x_train_001)
  x_train_003 = scaler_003.fit_transform(x_train_003)

  # testset FD001
  #df2.iloc[:, 2:] = scaler.transform(df2.iloc[:, 2:])


  #%%
  # Creating a sequence 
  # sequence_length = 26
  # forecast_length = 1
  # sequence_step = 1  # Schrittweite mit der das Fenster der Sequenzen verschoben wird

  # In einzelne Sequenzen aufteilen
  x = []
  y = []

  for i in range(fd001.shape[0]-sequence_length):
      if fd001.loc[i, "unit_nr_"] == fd001.loc[i+sequence_length, "unit_nr_"]:
        print('Train:', fd001.loc[i, "unit_nr_"])
        x.append(pd.DataFrame(x_train_001, columns=clm_to_use).iloc[i:i+sequence_length, 2:])
        # x.append(fd001.iloc[i:i+sequence_length, 2:])
        y.append(fd001.loc[i+sequence_length, 'RUL'])
      else:
        continue

  # at the end 
  x = np.array(x).reshape(-1, sequence_length, 14)
  y = np.array(y)

  # print("-----------")
  # print(x)
  print("Done!")
  # print(y)

  #%%
  # OPTION 1
  # Use dumps() to make it serialized
  # serialized = pickle.dumps(myvar)

  # OPTION2
  # Open a file and use dump()
  path_data = str(sequence_length)
  os.makedirs(path_data, exist_ok=True)
  with open(f'{sequence_length}/x_train_FD001.pkl', 'wb') as file:
      # A new file will be created
      pickle.dump(x, file)

  #%%

  with open(f'{sequence_length}/y_train_FD001.pkl', 'wb') as file:
      # A new file will be created
      pickle.dump(y, file)

  #%%

  # Use loads to load the variable
  # myvar = pickle.loads('x_train_FD001.pkl')

  # # Open the file in binary mode
  # with open('x_train_FD001.pkl', 'rb') as file:      
  #     # Call load method to deserialze
  #     x_train_001_pickle = pickle.load(file)


  #%%
  # TEST DATA

  file001_test = r'Data\test_FD001.txt'
  file003_test = r'Data\test_FD003.txt'
  rul001 = r'Data\RUL_FD001.txt'
  rul003 = r'Data\RUL_FD003.txt'

  test_fd1 = pd.read_csv(file001_test, sep=' ', header=None)
  test_fd3 = pd.read_csv(file003_test, sep=' ', header=None)

  rul_fd1 = pd.read_csv(rul001, sep=' ', header=None)
  rul_fd1.drop(1, axis=1, inplace=True)
  rul_fd3 = pd.read_csv(rul003, sep=' ', header=None)
  rul_fd3.drop(1, axis=1, inplace=True)

  # Drop columns 26 & 27 - they are empty
  for data in test_fd1, test_fd3:
    data.drop(columns=[26, 27], inplace=True)
    data.columns=columns

  #%%

  def add_rul_test(df, rul_df):
    """Add RUL column and values in the training data"""
    new_df = df.copy()
    new_df['RUL'] = np.zeros(new_df.shape[0])

    for i in df.groupby(df[df.columns[0]]): # returs tuple -> (id, data)
      row_index = np.where(df[df.columns[0]] == i[0])[0]
      # number of cycles before failure
      cycles = i[1][df.columns[1]].max() + rul_df[0][i[0]-1] 
      # creating reverse list of remaining cycles after each cycle
      values = np.arange(cycles-1, -1, -1)
      for row, value in zip(row_index, values):
        new_df.loc[row, 'RUL'] = 125 if value > 125 else value

    return new_df

  #%%
  fd001_test = add_rul_test(test_fd1, rul_fd1)
  fd003_test = add_rul_test(test_fd3, rul_fd3)

  # Data slicing
  x_test_001 = fd001_test[clm_to_use]
  x_test_003 = fd003_test[clm_to_use]

  # Target data
  y_test_001 = fd001_test.RUL
  y_test_003 = fd003_test.RUL

  # scale
  x_test_001 = scaler_001.transform(x_test_001)
  x_test_003 = scaler_003.transform(x_test_003)

  #%%
  # In einzelne Sequenzen aufteilen
  x_test = []
  y_test = []

  for i in range(fd001_test.shape[0]-sequence_length):
      if fd001_test.loc[i, "unit_nr_"] == fd001_test.loc[i+sequence_length, "unit_nr_"]:
        print(fd001_test.loc[i, "unit_nr_"])
        x_test.append(pd.DataFrame(x_test_001, columns=clm_to_use).iloc[i:i+sequence_length, 2:])
        # x.append(fd001_test.iloc[i:i+sequence_length, 2:])
        y_test.append(fd001_test.loc[i+sequence_length, 'RUL'])
      else:
        continue

  x_test = np.array(x_test).reshape(-1, sequence_length, 14)
  y_test = np.array(y_test)

  # print("-----------")
  # print(x)
  print("Done!")
  # %%
  # OPTION2
  # Open a file and use dump()
  with open(f'{sequence_length}/x_test_FD001.pkl', 'wb') as file:
      # A new file will be created
      pickle.dump(x_test, file)


  with open(f'{sequence_length}/y_test_FD001.pkl', 'wb') as file:
      # A new file will be created
      pickle.dump(y_test, file)

  # %%
  # Open the file in binary mode
  # with open('x_test_FD001.pkl', 'rb') as file:      
  #     # Call load method to deserialze
  #     x_test_pickle = pickle.load(file)

  # x_test_pickle

  # %%
  # RNN PART
  # LSTM ARCHITECTURE | 3 GATES
  # 3-5 LAYERS
  def LSTM_layer(n_layers=3):
    model = Sequential(

    )


  #%%
  # GRU ARCHITECTURE | 2 GATES
  # 3-5 LAYERS
  def GRU_layer():
    pass

  # make time sequence 
  # make sequences out of data / CO2 example 
  # R2 for evaluation

  #%% 
  # CNN PART 